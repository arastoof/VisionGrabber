using System.Diagnostics;
using System.IO;
using System.Net.Http;
using System.Text;
using Newtonsoft.Json;
using VisionGrabber.Utilities;

namespace VisionGrabber.Backends
{
    /// <summary>
    /// Implements a backend that runs and communicates with a local llama-server instance.
    /// </summary>
    public class LlamaBackend : IBackend
    {
        private Process _serverProcess;
        private readonly object _processLock = new object();


        /// <summary>
        /// Starts the local llama-server process using settings from SettingsManager.
        /// </summary>
        public void StartServer()
        {
            var settings = SettingsManager.Current;

            string llamaPath = settings.LocalLlamaPath;
            string modelPath = settings.LocalModelPath;
            string mmprojPath = settings.LocalMmprojPath;
            string port = settings.LocalLlamaPort;

            if (string.IsNullOrWhiteSpace(llamaPath) || !File.Exists(llamaPath)) 
            {
                System.Windows.MessageBox.Show($"Llama Server executable not found at: {llamaPath}\n\nPlease check your settings.", "Server Error", System.Windows.MessageBoxButton.OK, System.Windows.MessageBoxImage.Error);
                return;
            }
            if (string.IsNullOrWhiteSpace(modelPath) || !File.Exists(modelPath))
            {
                System.Windows.MessageBox.Show($"Model file not found at: {modelPath}\n\nPlease check your settings.", "Server Error", System.Windows.MessageBoxButton.OK, System.Windows.MessageBoxImage.Error);
                return;
            }

            lock (_processLock)
            {
                // Cleanup any existing instances
                CleanupAllServers();

                if (_serverProcess != null && !_serverProcess.HasExited) return;

                string contextSize = settings.LocalContextSize;
                if (string.IsNullOrWhiteSpace(contextSize)) contextSize = "2048";

                string arguments = $"-m \"{modelPath}\" --port {port} --host 0.0.0.0 -c {contextSize}";
                
                if (!string.IsNullOrWhiteSpace(mmprojPath) && File.Exists(mmprojPath))
                {
                    arguments += $" --mmproj \"{mmprojPath}\"";
                }

                var startInfo = new ProcessStartInfo
                {
                    FileName = llamaPath,
                    Arguments = arguments,
                    UseShellExecute = false,
                    CreateNoWindow = true,
                    WindowStyle = ProcessWindowStyle.Hidden,
                    RedirectStandardOutput = true,
                    RedirectStandardError = true
                };

                _serverProcess = Process.Start(startInfo);
                if (_serverProcess != null)
                {
                    _serverProcess.EnableRaisingEvents = true;
                    _serverProcess.Exited += (s, e) => 
                    {
                        lock (_processLock)
                        {
                            _serverProcess = null;
                        }
                    };
                }
            }
        }

        /// <summary>
        /// Terminates all running llama-server processes.
        /// </summary>
        private void CleanupAllServers()
        {
            try
            {
                var processes = Process.GetProcessesByName("llama-server");
                foreach (var p in processes)
                {
                    try { p.Kill(); p.WaitForExit(500); } catch { }
                }
            }
            catch { }
        }

        /// <summary>
        /// Stops the local server process and cleans up.
        /// </summary>
        public void StopServer()
        {
            lock (_processLock)
            {
                CleanupAllServers();
                _serverProcess = null;
            }
        }

        /// <summary>
        /// Sends an image processing request to the local llama-server.
        /// </summary>
        /// <param name="base64Image">The image encoded as a base64 string.</param>
        /// <param name="userPrompt">The prompt to send with the image.</param>
        /// <returns>The text generated by the model.</returns>
        public async Task<string> SendImageRequest(string base64Image, string userPrompt)
        {
            var settings = SettingsManager.Current;
            string port = settings.LocalLlamaPort;

            using (var client = new HttpClient())
            {
                var payload = new
                {
                    messages = new[]
                    {
                        new {
                            role = "user",
                            content = new object[]
                            {
                                new { type = "text", text = userPrompt },
                                new { type = "image_url", image_url = new { url = $"data:image/png;base64,{base64Image}" } }
                            }
                        }
                    },
                    temperature = 0.1, 
                    max_tokens = 500
                };

                var json = JsonConvert.SerializeObject(payload);
                var content = new StringContent(json, Encoding.UTF8, "application/json");

                try 
                {
                    var response = await client.PostAsync($"http://127.0.0.1:{port}/v1/chat/completions", content);
                    var responseString = await response.Content.ReadAsStringAsync();
                    
                    if (!response.IsSuccessStatusCode)
                    {
                        return $"Error: Server returned {response.StatusCode}";
                    }

                    dynamic result = JsonConvert.DeserializeObject(responseString);
                    
                    string text = result?.choices?[0]?.message?.content; 
                    return text ?? "No text found.";
                }
                catch (Exception ex)
                {
                    return "Error connecting to Llama: " + ex.Message;
                }
            }
        }
    }
}
